# CatchDesMoines Event Crawler
# Runs daily at 6 AM Central Time to crawl new events

name: Daily Event Crawler

on:
  # Schedule: Run daily at 6 AM Central Time (11 AM UTC in winter, 12 PM UTC in summer)
  schedule:
    # 12:00 UTC = 6 AM CDT (summer) or 7 AM CST (winter)
    - cron: '0 12 * * *'

  # Allow manual triggering
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum number of pages to crawl'
        required: false
        default: '5'
        type: string
      dry_run:
        description: 'Dry run (no database writes)'
        required: false
        default: false
        type: boolean

jobs:
  crawl-events:
    name: Crawl CatchDesMoines Events
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'crawlers/requirements.txt'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r crawlers/requirements.txt

      - name: Setup Crawl4AI
        run: |
          crawl4ai-setup
          crawl4ai-doctor || true

      - name: Install Playwright browsers
        run: |
          playwright install chromium
          playwright install-deps chromium

      - name: Run Event Crawler
        id: crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          CLAUDE_API: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          cd crawlers
          MAX_PAGES="${{ github.event.inputs.max_pages || '5' }}"
          DRY_RUN_FLAG=""
          if [ "${{ github.event.inputs.dry_run }}" == "true" ]; then
            DRY_RUN_FLAG="--dry-run"
          fi
          python catchdesmoines_crawler.py --max-pages "$MAX_PAGES" $DRY_RUN_FLAG

      - name: Post Summary
        if: always()
        run: |
          echo "## Event Crawler Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Run Time**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "- **Events Found**: ${{ steps.crawler.outputs.events_found || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Events Inserted**: ${{ steps.crawler.outputs.events_inserted || 'N/A' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Duplicates Skipped**: ${{ steps.crawler.outputs.duplicates_skipped || 'N/A' }}" >> $GITHUB_STEP_SUMMARY

      - name: Notify on Failure
        if: failure()
        run: |
          echo "::error::Event crawler failed! Check the logs for details."

  # Optional: Update AI Configuration in Supabase
  update-ai-config:
    name: Update AI Configuration
    runs-on: ubuntu-latest
    needs: crawl-events
    if: success() && github.event_name == 'workflow_dispatch'

    steps:
      - name: Update AI Model Setting
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: |
          # Update the default_model setting to Claude 4.5 Sonnet
          curl -X PATCH \
            "${SUPABASE_URL}/rest/v1/ai_configuration?setting_key=eq.default_model" \
            -H "apikey: ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
            -H "Content-Type: application/json" \
            -H "Prefer: return=minimal" \
            -d '{"setting_value": "claude-sonnet-4-5-20250929"}' \
            || echo "Note: AI config table may not exist yet"
