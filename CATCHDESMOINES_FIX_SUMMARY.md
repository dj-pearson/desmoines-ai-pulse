# CatchDesMoines URL Extraction - Critical Fix Applied

## Issue Identified from Your Logs

Looking at `scrape.md`, every event was showing:

```
‚ö†Ô∏è No external URL found
‚ö†Ô∏è No Visit Website found for "Event Name", using detail URL: https://www.catchdesmoines.com/event/...
```

This meant the extraction code was running but **failing to find the "Visit Website" buttons**.

## Root Cause

**CatchDesMoines.com is a JavaScript-rendered website.**

The "Visit Website" buttons are added to the page AFTER the initial HTML loads via JavaScript. Our code was using plain `fetch()` to retrieve event detail pages, which only returned the server-rendered HTML **without the JavaScript-rendered buttons**.

### Proof

When I tested with Puppeteer (which runs JavaScript), the button was clearly visible:

```html
<a
  href="https://www.facebook.com/IngersollNightmare"
  target="_blank"
  class="action-item"
>
  Visit Website
</a>
```

But when using `fetch()`, this button wasn't in the HTML at all.

## The Fix

Changed `extractCatchDesMoinesVisitWebsiteUrl()` function to use **Firecrawl API** instead of plain `fetch()`:

### Before (Broken)

```typescript
const response = await fetch(eventUrl, {
  headers: { "User-Agent": "..." },
});
const html = await response.text();
// ‚ùå Gets only server-rendered HTML, no JavaScript content
```

### After (Working)

```typescript
const firecrawlResponse = await fetch("https://api.firecrawl.dev/v1/scrape", {
  method: "POST",
  headers: { Authorization: `Bearer ${firecrawlApiKey}` },
  body: JSON.stringify({
    url: eventUrl,
    formats: ["html"],
    waitFor: 2000, // ‚úÖ Waits for JavaScript to execute
  }),
});
const html = firecrawlData.data?.html;
// ‚úÖ Gets fully-rendered HTML with all dynamic content
```

### Additional Improvements

1. **Flexible Regex Patterns**: Updated regex to handle varying attribute orders in HTML
2. **Better Error Logging**: Added check for empty HTML responses
3. **Mailto Filter**: Excludes `mailto:` links (like "Send an Email" buttons)

## Cost Impact

- **Before**: 3 Firecrawl API calls (3 list pages)
- **After**: ~33 Firecrawl API calls (3 list pages + 30 event detail pages)

**Why it's worth it**: Users get actual venue/event websites instead of CatchDesMoines URLs, providing much more value.

## Files Changed

1. **`supabase/functions/firecrawl-scraper/index.ts`**

   - Lines 36-120: Updated `extractCatchDesMoinesVisitWebsiteUrl()` to use Firecrawl
   - Line 712: Added `firecrawlApiKey` parameter to function call
   - Improved regex patterns for better HTML matching

2. **`CATCHDESMOINES_URL_EXTRACTION.md`**
   - Updated documentation with root cause analysis
   - Added cost consideration section
   - Explained JavaScript rendering issue

## Testing

Next time you run the "Catch Des Moines Events" scraping job, you should see:

```
üîó Processing 35 events to extract Visit Website URLs...
üîç Extracting Visit Website URL from: https://www.catchdesmoines.com/event/...
‚úÖ Found Visit Website URL: https://www.facebook.com/...
‚úÖ Found Visit Website URL: https://prariemeadows.com/...
‚úÖ Extracted Visit Website URL for "Event Name": https://venue.com
‚úÖ Completed URL extraction for 35 events
```

Instead of:

```
‚ö†Ô∏è No external URL found  ‚ùå (old behavior)
```

## Database Impact

Events will now have:

- **source_url**: External venue/event website (e.g., Facebook event, venue website)
- **NOT**: CatchDesMoines.com URLs

This provides users with direct links to get tickets, RSVP, or learn more about the event from the source.

## Rate Limiting Issue & Fix (CRITICAL UPDATE)

### New Problem: HTTP 429 Errors

After deploying the Firecrawl-based extraction, we hit **Firecrawl's rate limits**. All requests were failing with:

```
‚ùå Firecrawl error: 429
```

**Why?** We were making 35 simultaneous Firecrawl API calls (using `Promise.all()`), which exceeded Firecrawl's rate limits.

### Solution: Sequential Processing with Delays

Changed from **parallel** to **sequential** processing with 1-second delays between requests:

**Before (Broken - Rate Limited):**

```typescript
filteredItems = await Promise.all(
  filteredItems.map(async (item) => {
    // All requests fire simultaneously ‚ùå
    const visitWebsiteUrl = await extractCatchDesMoinesVisitWebsiteUrl(...);
  })
);
```

**After (Working - Rate Limited):**

```typescript
// Process events one at a time with 1-second delays
for (let i = 0; i < filteredItems.length; i++) {
  const visitWebsiteUrl = await extractCatchDesMoinesVisitWebsiteUrl(...);

  // Wait 1 second before next request ‚úÖ
  if (i < filteredItems.length - 1) {
    await new Promise(resolve => setTimeout(resolve, 1000));
  }
}
```

### Performance Impact

- **35 events** = ~35 seconds total processing time (1 second per event)
- **Trade-off**: Slower processing, but successfully avoids rate limits
- **Progress tracking**: Now shows `[1/35]`, `[2/35]`, etc. in logs

## Ready to Deploy

The fix is complete with rate limiting. Run your next scraping job and you should see:

1. ‚úÖ Sequential processing with progress indicators
2. ‚úÖ Actual venue URLs being extracted
3. ‚úÖ No more 429 errors!

**Expected runtime**: ~35-40 seconds for 35 events (vs 5 seconds parallel, but that was getting rate limited)
